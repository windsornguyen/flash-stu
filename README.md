# ‚ö°Ô∏è Flash STU ‚ö°Ô∏è

<div align="center">
  <img src="docs/flash-stu.webp" alt="Flash STU Logo" width="720">
</div>

## Table of Contents
1. [Introduction](#introduction)
2. [Features](#features)
3. [Installation](#installation)
4. [Usage](#usage)
5. [Configuration](#configuration)
6. [Contributing](#contributing)
7. [License](#license)
8. [Acknowledgments](#acknowledgments)

## Introduction

This repository complements the [Flash STU: Fast Spectral Transform Units](https://arxiv.org/abs/2409.10489) paper and contains an optimized, open-source PyTorch implementation of the Spectral Transform Unit (STU) as proposed in [*Spectral State Space Models*](https://arxiv.org/abs/2312.06837) by Agarwal et al. (2024).

The [STU](stu.py) module is a fast and flexible building block that can be adapted into a wide range of neural network architectures, especially those that aim to solve tasks with long-range dependencies.

## Features

- ‚ö°Ô∏è Fast convolutions using [Flash FFT](https://github.com/HazyResearch/flash-fft-conv)
- üöÄ Fast, local attention using (sliding window) [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- üåê Support for distributed training using [DDP](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) and [FSDP](https://pytorch.org/docs/stable/fsdp.html)

## Installation

> **Note**: CUDA is required to run code from this repository.

This repository was tested with:
- Python 3.12.5
- PyTorch 2.4.0
- Triton 3.0.0
- CUDA 12.4

and may be incompatible with other versions.

1. Install PyTorch with CUDA support:
    ```bash
    pip install torch --index-url https://download.pytorch.org/whl/cu124
    ```

2. Install Flash Attention:
   ```bash
   pip install ninja
   MAX_JOBS=4 pip install flash-attn --no-build-isolation
   ```

3. Install Flash FFT:
   ```bash
    pip install git+https://github.com/HazyResearch/flash-fft-conv.git#subdirectory=csrc/flashfftconv
    pip install git+https://github.com/HazyResearch/flash-fft-conv.git
    ```

4. Install Flash STU:
   ```bash
   pip install -e .
   ```

## Usage

### Using FlashSTU

You can start using a FlashSTU (by initializing it from scratch) in the following way:
``` python
from flashstu import FlashSTU, FlashSTUConfig, get_spectral_filters
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

config = FlashSTUConfig(
      MODIFY_YOUR_ARGS_HERE
    )
phi = get_spectral_filters(config.seq_len, config.num_eigh, config.use_hankel_L, device, config.torch_dtype)
model = FlashSTU(
   config, 
   phi
)
y = model(x)
```

### If you want to replicate the training

First of all, once you have followed the installation steps, cd into `training` dir.

An example training script for pretraining a large language model is provided in [`example.py`](example.py).

If your compute cluster does not have internet access, you will need to pre-download the entire dataset before running the example script.

To download the dataset, run:
```bash
python data.py
```

We provide an LLM pretraining script, [`example.py`](example.py), for you to test out the repository. To begin training, run the following commands in your terminal. Make sure to adapt the script if you are not using [environement modules](https://modules.readthedocs.io/en/latest/index.html):

```bash
chmod +x job.slurm
./job.slurm
```

If you are in a compute cluster that uses Slurm and [environement modules](https://modules.readthedocs.io/en/latest/index.html), you can submit a job using the following command:
```bash
sbatch job.slurm
```

Be sure to adjust the configurations of the [Slurm job](job.slurm) based on your cluster's constraints.

> **Note**: The FineWeb-Edu 10B-token sample is a relatively large dataset. It can be swapped out for something smaller, e.g. [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) (476.6M tokens).


#### Training configuration

Training configurations can be adjusted as needed in [`config.json`](config.json).

> **Note**: PyTorch's `torch.compile` currently does not have great support for distributed wrapper modules like DDP or FSDP. If you encounter errors during training, try disabling `torch.compile`. For more information on `torch.compile`, see this [informal manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.ivdr7fmrbeab).


## Contributing

Contributions are welcomed! Writing performant distributed code is always tricky. We welcome contributors to:

- Submit pull requests
- Report issues
- Help improve the project overall

## License

Apache 2.0 License

You can freely use, modify, and distribute the software, **even in proprietary products**, as long as you:
- Include proper attribution
- Include a copy of the license
- Mention any changes made

It also provides an express grant of patent rights from contributors.

See the [LICENSE](LICENSE) file for more details.

## Acknowledgments

Huge thanks to (in no particular order):
- Elad Hazan and the authors of the [Spectral State Space Models](https://arxiv.org/abs/2312.06837) paper
- Isabel Liu, Yagiz Devre, Evan Dogariu
- The Flash Attention team
- The Flash FFT team
- The PyTorch team
- Princeton Research Computing and Princeton Language and Intelligence, for supplying compute
- Andrej Karpathy, for his awesome [NanoGPT](https://github.com/karpathy/build-nanogpt) repository

## Citation

If you use this codebase, or otherwise find our work valuable, please cite Flash STU:
```
@article{Flash STU,
  title={Flash STU: Fast Spectral Transform Units},
  author={Isabel Y. Liu and Windsor Nguyen and al.},
  journal={arXiv preprint arXiv:2409.10489v2},
  year={2024}
}
